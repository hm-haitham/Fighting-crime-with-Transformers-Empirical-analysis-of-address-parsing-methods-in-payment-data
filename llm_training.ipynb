{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b022907c-7d63-4610-bb85-ba2d08a5b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5614c4-338d-4b52-9db5-8b2af6fa8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540192d1-2e2a-4b12-9007-09a1ffa2e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input expects a column named \"sentence\" containing a list of strings corresponding to space-separated words\n",
    "# The input expects a column named \"tags\" with target fields (same length as \"sentence\")\n",
    "df = pickle.load(open(\"data/V2_train.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6610fe-9aac-441c-a9d1-cb01cf4b6198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment from the list below the model to use\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# model_id = \"meta-llama/Llama-2-7b\"\n",
    "# model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    max_length=3000\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    is_split_into_words=True,\n",
    "    padding_side=\"right\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf4598-d353-4fee-9b9b-b4d2a44ba838",
   "metadata": {},
   "outputs": [],
   "source": [
    "template =\"\"\"<s>[INST]\n",
    "You are a word classifier that classifies words from a text corresponding to an address free text field.\n",
    "You should analyze with deep precision the INPUT and return a dictionary with the following keys: \"Name\", \"StreetNumber\", \"StreetName\", \"Municipality\", \"PostalCode\", \"Unit\", \"Country\", \"CountryCode\".\n",
    "Each word is separated by a space and should be classified without any modification.\n",
    "Each word in the input has a prefix with the index i as '[i]-' and it should be ignored for the classification but it should remain AS-IS in the output.\n",
    "Sub sequence of words should be classified as follow:\n",
    "'Name': words corresponding to an indiviual name or institution name.\n",
    "'StreetNumber': words corresponding to a street number.\n",
    "'StreetName': words corresponding to a street name.\n",
    "'Municipality': words corresponding to a municipality or city.\n",
    "'PostalCode': words corresponding to a postal code.\n",
    "'Unit': words corresponding to a unit number.\n",
    "'Country': words corresponding to a full country name.\n",
    "'CountryCode': words corresponding to a country iso2 code.\n",
    "\n",
    "Output Indicator:\n",
    "1. If you are not sure about one word, don't classify it.\n",
    "2. Usually a name comes before the address.\n",
    "3. \"$\" is indicating a large separator and it should not be classified.\n",
    "4. The output words should be taken from the input only and it should not be modified\n",
    "5. The same word cannot be used in two different classes.\n",
    "6. Words are classified subsequently.\n",
    "7. Empty classes should not appear in the output.\n",
    "8. Output should not include nested values.\n",
    "9. Each index are taken from the input itself and the index matches, e.g. the prefix '[i]-' remains unchanged for all words.\n",
    "\n",
    "For example:\n",
    "### INPUT:\n",
    "\"[0]-THOMASSEN [1]-GULBRANDSEN [2]-OG [3]-GUNDERSEN [4]-$ [5]-TV [6]-SD [7]-9 [8]-JAPARATINGA [9]-57950 [10]-000 [11]-BR\"\n",
    "### OUTPUT: \n",
    "{\"Name\": \"[0]-THOMASSEN [1]-GULBRANDSEN [2]-OG [3]-GUNDERSEN\", \"StreetName\": \"[5]-TV [6]-SD [7]-9\", \"Municipality\": \"[8]-JAPARATINGA\", \"PostalCode\": \"[9]-57950 [10]-000\", \"CountryCode\": \"[11]-BR\"}\n",
    "\n",
    "\n",
    "### INPUT:\n",
    "{full_address}\n",
    "[/INST]\n",
    "\n",
    "### OUTPUT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74370e4-d455-4cd0-bb54-b73a925a055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output(row):\n",
    "  d = defaultdict(str)\n",
    "  for i, tag in enumerate(row[\"tags\"]):\n",
    "      try:\n",
    "          d[tag[2:]] += \" \" + row[\"full_address\"].split(\" \")[i]\n",
    "      except:\n",
    "          print(row)\n",
    "          print(d)\n",
    "  return d\n",
    "\n",
    "# Concat prefix index\n",
    "df[\"full_address\"] = df.apply(lambda row: \" \".join(\"[\" + str(i) + \"]-\" + token for i, token in enumerate(row[\"sentence\"])), axis=1)\n",
    "# Create dictionary for target\n",
    "df[\"target_output\"] = df.apply(create_output, axis=1)\n",
    "data = Dataset.from_pandas(df)\n",
    "\n",
    "# Create training prompt\n",
    "data = data.map(lambda example: {\"prompt\": f\"{template}\\n{example['target_output']}\"})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f568b89-d741-446f-9b5a-cab8c4df99cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        eval_accumulation_steps=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,\n",
    "        warmup_steps=0.03,\n",
    "        max_steps=-1,\n",
    "        learning_rate=2e-4,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        logging_steps=10,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_32bit\",  \n",
    "        save_strategy=\"steps\",\n",
    "        log_level=\"info\",\n",
    "        logging_first_step=True\n",
    "    ),\n",
    "    max_seq_length=3000,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224077b-0715-4f91-8ee7-0323dc39e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"path/to/model\"\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
